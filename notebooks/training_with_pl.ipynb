{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'segmentation_models_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ce4e55130f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_2d_or_3d_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation_models_pytorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segmentation_models_pytorch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import monai \n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Invertd,\n",
    "    Resized,\n",
    "    EnsureTyped,\n",
    "    SqueezeDimd,\n",
    "    AddChannel,\n",
    "    EnsureType\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch._tensor\n",
    "from pathlib import Path\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "import torchvision.models as models\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import wandb \n",
    "from torchmetrics.functional import accuracy, dice_score, average_precision, matthews_corrcoef, precision_recall, jaccard_index\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "import pydicom as dicom\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import wandb \n",
    "from pathlib import Path \n",
    "from SarcoDataModule import SarcoDataModule\n",
    "#from SarcoInfDataModule import SarcoInfDataModule\n",
    "from manual_cropping import *\n",
    "from data_augment import *\n",
    "from monai.transforms import MapTransform, InvertibleTransform, Resize\n",
    "import itertools\n",
    "from itertools import product\n",
    "import dicom2nifti as d2n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining some metrics \n",
    "\n",
    "SMOOTH = 1e-6\n",
    "\n",
    "def iou_and_dice_pytorch(output: torch.Tensor, label: torch.Tensor):\n",
    "\n",
    "    outputs = torch.flatten(torch.squeeze(output, dim=1))  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    labels = torch.flatten(torch.squeeze(label,dim=1))\n",
    "    \n",
    "    intersection = abs(float(outputs.dot(labels)))\n",
    "\n",
    "    union = abs(float(torch.sum(labels))+ float(torch.sum(outputs)) - intersection  )  \n",
    "    sum_pixels= torch.sum(outputs) + torch.sum(labels)\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    dice= 2*(intersection/sum_pixels)\n",
    "        \n",
    "    return iou, dice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try collect directly nii files and dicom files\n",
    "dirpath_raw=Path('/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved')\n",
    "problem_path='blabla'\n",
    "files_raw=[filename for filename in dirpath_raw.glob('*') if filename != problem_path]\n",
    "data=[]\n",
    "for k in range(0,len(sorted(files_raw)),2):\n",
    "    input_dict=dirpath_raw/ Path(sorted(files_raw)[k])\n",
    "    label_dict=dirpath_raw/ Path(sorted(files_raw)[k+1])\n",
    "    data.append({\"input\":input_dict ,\"label\":label_dict})\n",
    "\n",
    "data\n",
    "\n",
    "transform=LoadImaged(keys=['input', 'label'], dtype='float32')\n",
    "ex_ds=monai.data.Dataset(data=data, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m wandb\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49msummary\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n"
     ]
    }
   ],
   "source": [
    "#collecting data and splitting between train and val \n",
    "dirpath=Path('data/sarco_final1')\n",
    "problem_path=Path('/home/ghita/milvuetap/ghita_exploration/data/sarco_final1/HyuyQN0Tca')\n",
    "files=[filename for filename in dirpath.glob('*') if filename != problem_path]\n",
    "print(len(files))\n",
    "data=[]\n",
    "for k in range(len(sorted(files))):\n",
    "    filename=sorted(files)[k].name.replace('.dcm', '')\n",
    "    input_dict=dirpath/filename/f\"{filename}.dcm.npy\"\n",
    "    label_dict=dirpath/filename/f\"{filename}.nii.npy\"\n",
    "    data.append({\"input\":input_dict ,\"label\":label_dict})\n",
    "\n",
    "\n",
    "train_files, val_files=train_test_split(data,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms=Compose(\n",
    "    [LoadImaged(keys=[\"input\"], reader='numpyreader', dtype='float32', image_only=True),\n",
    "    LoadImaged(keys=[\"label\"], reader='numpyreader', dtype='float32', image_only=True),\n",
    "    SqueezeDimd(keys=['label'],dim=2),\n",
    "    AddChanneld(keys=[\"input\",\"label\" ]),\n",
    "    Resized(keys=[\"input\"],spatial_size=(512,512)),\n",
    "    Resized(keys=[\"label\"], spatial_size=(512,512))\n",
    "    ]\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmonai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m     AddChanneld,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m     Compose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=7'>8</a>\u001b[0m     SqueezeDimd,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=10'>11</a>\u001b[0m ex_dataset\u001b[39m=\u001b[39mmonai\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset(data\u001b[39m=\u001b[39mtrain_files, transform\u001b[39m=\u001b[39mtransforms)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=11'>12</a>\u001b[0m prod\u001b[39m=\u001b[39mProductDataset(ex_dataset, ex_dataset, transform\u001b[39m=\u001b[39mCompose(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=12'>13</a>\u001b[0m                         [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=13'>14</a>\u001b[0m                             Mixup_nobatchd(keys\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m], alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=21'>22</a>\u001b[0m                         ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000019vscode-remote?line=22'>23</a>\u001b[0m                     ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    SqueezeDimd,\n",
    ")\n",
    "from utils.data_augment import ProductDataset, Mixupd\n",
    "\n",
    "ex_dataset=monai.data.Dataset(data=train_files, transform=transforms)\n",
    "prod=ProductDataset(ex_dataset, ex_dataset, transform=Compose(\n",
    "                        [\n",
    "                            Mixupd(keys=[\"input\", \"label\"], alpha=1),\n",
    "                            RandRotate90d(\n",
    "                                keys=[\"input\", \"label\"], prob=0.5, spatial_axes=[0, 1]\n",
    "                            ),\n",
    "                            EnsureTyped(\n",
    "                                keys=[\"input\"], data_type=\"tensor\", dtype=torch.float\n",
    "                            ),\n",
    "                            EnsureTyped(keys=[\"label\"], data_type=\"tensor\"),\n",
    "                        ]\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:95: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:114: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "module = SarcoDataModule(data_dir=Path('data/sarco_final1'), \n",
    "do_mixup=False, batch_size=16, \n",
    "crop_pixel_width=1, aug_prob=0.2, \n",
    "aug_sigma=7, aug_alpha=35)\n",
    "module.setup()\n",
    "time_dataloader=module.train_dataloader()\n",
    "train_ds = module.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dl):\n",
    "    # assert len(time_dataloader) > 10, len(time_dataloader)\n",
    "    for _ in range(1):\n",
    "        for (i, _) in enumerate(dl):\n",
    "            if i > 84:\n",
    "                break  \n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 40, in fetch\n    return self.collate_fn(data)\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 157, in default_collate\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 157, in <dictcomp>\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 146, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 146, in <listcomp>\n    return default_collate([torch.as_tensor(b) for b in batch])\nValueError: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb Cell 7'\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(dl)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000020vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbenchmark\u001b[39m(dl):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000020vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39m# assert len(time_dataloader) > 10, len(time_dataloader)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000020vscode-remote?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000020vscode-remote?line=3'>4</a>\u001b[0m         \u001b[39mfor\u001b[39;00m (i, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dl):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000020vscode-remote?line=4'>5</a>\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m84\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000020vscode-remote?line=5'>6</a>\u001b[0m                 \u001b[39mbreak\u001b[39;00m  \n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1221'>1222</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1222'>1223</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1223'>1224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1247'>1248</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1248'>1249</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1249'>1250</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=1250'>1251</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/_utils.py?line=452'>453</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/_utils.py?line=453'>454</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/_utils.py?line=454'>455</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/_utils.py?line=455'>456</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/_utils.py?line=456'>457</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 40, in fetch\n    return self.collate_fn(data)\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 157, in default_collate\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 157, in <dictcomp>\n    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 146, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 146, in <listcomp>\n    return default_collate([torch.as_tensor(b) for b in batch])\nValueError: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) \n"
     ]
    }
   ],
   "source": [
    "%time benchmark(time_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, batch_size, alpha):\n",
    "    mixed_data=[]\n",
    "    for k in range(len(data) // batch_size):\n",
    "        if len(data) < (k + 1) * batch_size:\n",
    "            batch_data = data[k * batch_size :]\n",
    "        else:\n",
    "            batch_data = data[(k * batch_size) : ((k + 1) * batch_size)]\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1.0\n",
    "        index = torch.randperm(batch_size)\n",
    "        randomized=[batch_data[ind] for ind in index]\n",
    "        \n",
    "        for k in range(len(batch_data)):\n",
    "            l=0\n",
    "            if batch_data[k].shape==randomized[k].shape:\n",
    "                mixed_data.append(np.multiply(batch_data[k], lam) + np.multiply(randomized[k],(1 - lam)))\n",
    "                l+=1\n",
    "            else :\n",
    "                mixed_data.append(batch_data[k])\n",
    "    return mixed_data,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=[{'input': [1,2,3], 'label':['a','b','c']},{'input': [3,4,5], 'label':['d','e','f']},{'input': [6,7,8], 'label':['g','h','i']} ]\n",
    "'''aug_ex=[]\n",
    "for k in range(len(ex)):\n",
    "    for key in dict(ex):\n",
    "        aug_ex.append(f\"{key}_right\":ex[k][key], f\"{key}_left\":)\n",
    "list_r=itertools.product(ex,2)\n",
    "for el in list_r:\n",
    "    print(el)'''\n",
    "#combis=list(itertools.combinations(ex,2))\n",
    "#combis\n",
    "#dico=dict([(k,[combis[0][0],combis[0][1]]) for k in combis[0][0]])\n",
    "#print(combis)\n",
    "#for k in range(len(combis)):\n",
    "#    combis[k][0]['input_right']=combis[k][0].pop('input')\n",
    "#    combis[k][1]['label_right']=combis[k][0].pop('label')\n",
    "\n",
    "#combis\n",
    "#for k in range(len(combis)):\n",
    "ex_dataset=monai.data.Dataset(data=ex)\n",
    "prod=ProductDataset(ex_dataset, ex_dataset)\n",
    "list(prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_product(dict_dataset:list, length_tuples:int=2):\n",
    "    \"\"\"\n",
    "    This function takes a list of dictionaries dataset and creates another dataset based on the \n",
    "    first one but where each key contains not one element but a combination of elements from the original dataset.\n",
    "\n",
    "    Inputs : \n",
    "\n",
    "    dict_dataset : [{'key_a': element_a, 'key_b': element_b}, {...}]\n",
    "\n",
    "    length_tuples : number of elements wanted for each key. length_tuples=1 -> the function does nothing, length_tuples=2 -> the \n",
    "    function creates a cartesian product of the original dataset with no repeated elements. length_tuples=len(dict_dataset) -> all \n",
    "    the elements of the dict_dataset are concatenated. \n",
    "\n",
    "    Output : \n",
    "\n",
    "    Combinations of the input : [{'key_a_0' : element_a_0, 'key_b_0': element_b_0, \n",
    "    'key_a_1': element_a_1, 'key_b_1': element_b_1, ..., 'key_a_length_tuples': element_a_length_tuples, \n",
    "    'key_b_length_tuples': element_b_length_tuples}, {...}]\n",
    "    \"\"\"\n",
    "    keys=dict(dict_dataset[0])\n",
    "    output=[]\n",
    "    combis=list(itertools.combinations(dict_dataset,length_tuples))\n",
    "    for combi in combis:\n",
    "        element=[]\n",
    "        for k in range(length_tuples):\n",
    "            for key in keys:\n",
    "                element.append(f\"{key}_%s\"%k)\n",
    "                element.append(combi[k][key])\n",
    "        new_keys=element[0::2]\n",
    "        values=element[1::2]\n",
    "        zip_iterator=zip(new_keys, values)\n",
    "        output.append(dict(zip_iterator))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       "  'label': array([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]])},\n",
       " {'input': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       "  'label': array([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       "  \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]])}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_batch=monai.data.Dataset(data=train_files[:250])\n",
    "prod=ProductDataset(ex_batch,ex_batch)\n",
    "list(prod)[0]\n",
    "#right, left=list(prod)[1]\n",
    "#mixing=Mixupd(keys=['input'], alpha=1)\n",
    "#mixed=mixing(prod)\n",
    "#plt.imshow(mixed['input'])\n",
    "#plt.imshow(list(prod)[5][0]['input'])\n",
    "#print(mixed['input']==list(prod)[5][0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixing=Mixupd(['input', 'label'], 1)\n",
    "mixing(ex_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixing=Mixupd(keys=[\"input\", \"label\"], alpha=1, batch_size=50)\n",
    "#inputs=[np.reshape(ex_batch[k][\"input\"],(512,512)) for k in range(len(ex_batch))]\n",
    "#labels=[ex_batch[k][\"label\"] for k in range(len(ex_batch))]\n",
    "#inputs[0], np.multiply(inputs[0], 3)\n",
    "\n",
    "#a=mixup(inputs,batch_size=5,alpha=0.3)\n",
    "#a[1]\n",
    "#type(a[0])\n",
    "#plt.imshow(a[0][0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the productdatset class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the pl Data Module\n",
    "class SarcoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=Path('data/sarco_final1')):\n",
    "        super().__init__()\n",
    "        self.data_dir=data_dir\n",
    "        self.train_transforms=Compose(\n",
    "                [LoadImaged(keys=[\"input\"], reader='numpyreader', dtype='float32', image_only=True),\n",
    "                LoadImaged(keys=[\"label\"], reader='numpyreader', dtype='float32', image_only=True),\n",
    "                SqueezeDimd(keys=['label'],dim=2),\n",
    "                AddChanneld(keys=[\"input\",\"label\" ]),\n",
    "                Resized(keys=[\"input\"],spatial_size=(512,512)),\n",
    "                Resized(keys=[\"label\"], spatial_size=(512,512)),\n",
    "                #RandRotate90d(keys=[\"input\", \"label\"], prob=0.5, spatial_axes=[0,1]),\n",
    "                #Mixupd(keys=[\"input\", \"label\"], alpha=1, batch_size=20),\n",
    "                #EnsureTyped(keys=[\"input\"],data_type=\"tensor\",dtype=torch.float),\n",
    "                #EnsureTyped(keys=['label'],data_type=\"tensor\"),\n",
    "                ]\n",
    "            )  \n",
    "        self.val_transforms=Compose(\n",
    "                [\n",
    "                LoadImaged(keys=[\"input\"], reader='numpyreader', dtype='float32', image_only=True),\n",
    "                LoadImaged(keys=[\"label\"], reader='numpyreader', dtype='float32', image_only=True),\n",
    "                SqueezeDimd(keys=['label'],dim=2),\n",
    "                AddChanneld(keys=[\"input\",\"label\" ]),\n",
    "                Resized(keys=[\"input\"],spatial_size=(512,512)),\n",
    "                Resized(keys=[\"label\"], spatial_size=(512,512)),\n",
    "                EnsureTyped(keys=[\"input\"],data_type=\"tensor\",dtype=torch.float),\n",
    "                EnsureTyped(keys=['label'],data_type=\"tensor\"),\n",
    "                ]\n",
    "            )  \n",
    "        self.predict_transforms=Compose(AddChannel(), EnsureType(data_type='tensor'))\n",
    "    def setup(self, stage=None):\n",
    "        if stage=='fit' or stage is None:\n",
    "            files=[filename for filename in self.data_dir.glob('*') if filename != Path('/home/ghita/milvuetap/ghita_exploration/data/sarco_final1/HyuyQN0Tca')]\n",
    "            data=[]\n",
    "            for k in range(len(sorted(files))):\n",
    "                filename=sorted(files)[k].name.replace('.dcm', '')\n",
    "                input_dict=self.data_dir/filename/f\"{filename}.dcm.npy\"\n",
    "                label_dict=self.data_dir/filename/f\"{filename}.nii.npy\"\n",
    "                data.append({\"input\":input_dict ,\"label\":label_dict})\n",
    "            self.train_files, self.val_files=train_test_split(data,test_size=0.2)\n",
    "            self.val_files, self.test_files=train_test_split(self.val_files, test_size=0.5)\n",
    "            self.train_ds=monai.data.Dataset(data=self.train_files[:250], transform=self.train_transforms)\n",
    "            mixing=Mixupd(keys=['input', 'label'], alpha=1)\n",
    "            self.train_ds=mixing(ProductDataset(self.train_ds, self.train_ds))\n",
    "            self.train_ds=monai.data.Dataset(data=self.train_ds, transform=EnsureTyped(keys=[\"input\", \"label\"],data_type=\"tensor\",dtype=torch.float))\n",
    "            self.val_ds=monai.data.Dataset(data=self.val_files, transform=self.val_transforms)\n",
    "        if stage=='test':\n",
    "            self.test_ds=monai.data.Dataset(data=self.test_files, transform=self.val_transforms)\n",
    "        if stage=='predict':\n",
    "            to_infer_nifti=nib.load('/home/ghita/milvuetap/ghita_exploration/data/CT.1.2.840.113619.2.416.9243487930095034124053722995226898598.nii')\n",
    "            infer_slices=[]\n",
    "            for k in range(to_infer_nifti.get_fdata().shape[2]):\n",
    "                infer_slices.append(to_infer_nifti.get_fdata()[:,:,k])\n",
    "            self.pred_ds=monai.data.Dataset(data=infer_slices, transform=self.predict_transforms)\n",
    "    def train_dataloader(self):\n",
    "        return(DataLoader(self.train_ds,batch_size=8,num_workers=4))\n",
    "    def val_dataloader(self):\n",
    "        return(DataLoader(self.val_ds,batch_size=8,num_workers=4))\n",
    "    def test_dataloader(self):\n",
    "        return(DataLoader(self.test_ds, batch_size=8, num_workers=4))\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.pred_ds, batch_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the pl module\n",
    "class pl_UNet(pl.LightningModule):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model=smp.Unet(encoder_name='efficientnet-b0',encoder_weights='imagenet',in_channels=1,classes=1)\n",
    "        self.learning_rate=learning_rate\n",
    "    def forward(self,x):\n",
    "        out=self.model(x)\n",
    "        return(out) \n",
    "    def training_step(self,train_batch,batch_nb):\n",
    "        input=train_batch['input']\n",
    "        label=train_batch['label']\n",
    "        pred=self.forward(input)\n",
    "        dice=iou_and_dice_pytorch(torch.sigmoid(pred), label)[1]\n",
    "        loss=monai.losses.DiceLoss(include_background=False, sigmoid=True)\n",
    "        loss_value=loss(pred,label.float())\n",
    "        iou_value=iou_and_dice_pytorch(torch.sigmoid(pred),label)[0]\n",
    "        acc=accuracy(pred,label.type(torch.int))\n",
    "        prec=average_precision(pred, label)\n",
    "        logger.log_image(key='prediction', images=[predi for predi in pred])\n",
    "        logger.log_image(key='input', images=[inp for inp in input])\n",
    "        logger.log_image(key='label', images=[pr for pr in label])\n",
    "        self.log('IoU_train', iou_value)\n",
    "        self.log('loss_train',loss_value)\n",
    "        self.log('acc_train',acc)\n",
    "        self.log('Dice_train', dice)\n",
    "        self.log('Avrg Precision_train', prec)\n",
    "        return(loss_value)\n",
    "    def validation_step(self, val_batch,batch_nb):\n",
    "        input=val_batch['input']\n",
    "        label=val_batch['label']\n",
    "        pred=self.forward(input)\n",
    "        loss=monai.losses.DiceLoss(include_background=False, sigmoid=True)\n",
    "        loss_value=loss(pred, label.float())\n",
    "        iou_value=iou_and_dice_pytorch(torch.sigmoid(pred),label)[0]\n",
    "        dice=iou_and_dice_pytorch(torch.sigmoid(pred), label)[1]\n",
    "        acc=accuracy(pred,label.type(torch.int))\n",
    "        prec=average_precision(pred, label)\n",
    "        logger.log_image(key='prediction_val', images=[predi for predi in pred])\n",
    "        logger.log_image(key='input_val', images=[inp for inp in input])\n",
    "        logger.log_image(key='label_val', images=[pr for pr in label])\n",
    "        self.log('IoU_val',iou_value)\n",
    "        self.log('loss_val',loss_value)\n",
    "        acc=accuracy(pred,label.type(torch.int))\n",
    "        self.log('acc_val',acc)\n",
    "        self.log('Dice_val', dice)\n",
    "        self.log('Avrg Precision_ val', prec)\n",
    "        return(loss_value)\n",
    "    def test_step(self, test_batch, batch_nb):\n",
    "        return None\n",
    "    def predict_step(self, batch, batch_nb):\n",
    "        x=batch.type(torch.float)\n",
    "        pred=self(x)\n",
    "        return(pred)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.Adam(self.parameters(),lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train \n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=pl_UNet(learning_rate=1e-3).to(device)\n",
    "run = wandb.init(project=\"train\",settings=wandb.Settings(start_method='thread'))\n",
    "checkpoint_callback_best=ModelCheckpoint(dirpath='/home/ghita/milvuetap/ghita_exploration/checkpoints', monitor='loss_val',save_top_k=1,mode='min')\n",
    "best_checkpoint_path=checkpoint_callback_best.best_model_path\n",
    "checkpoint_callback_last=ModelCheckpoint(dirpath='/home/ghita/milvuetap/ghita_exploration/checkpoints', monitor='step',save_top_k=1,mode='max')\n",
    "last_checkpoint_path=checkpoint_callback_last.best_model_path\n",
    "logger=WandbLogger(project='train')\n",
    "trainer=pl.Trainer(logger=logger,max_epochs=1,auto_lr_find=True,auto_scale_batch_size=True, callbacks=[checkpoint_callback_best, checkpoint_callback_last], accelerator='gpu', gpus=1)\n",
    "trainer.fit(model, datamodule=SarcoDataModule(data_dir=Path('data/sarco_final1'), augment=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"train\", settings=wandb.Settings(start_method='fork'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether the inverse functions work \n",
    "\n",
    "example_dict=[{\"input\": Path(\"/home/ghita/milvuetap/ghita_exploration/data/sarco_final1/0bzRUjziqI/0bzRUjziqI.dcm.npy\"), \"label\": Path(\"/home/ghita/milvuetap/ghita_exploration/data/sarco_final1/0bzRUjziqI/0bzRUjziqI.nii.npy\")}]\n",
    "transforms=Compose([LoadImaged(keys=[\"input\", \"label\"], reader=\"numpyreader\",\n",
    "                    dtype=\"float32\",\n",
    "                    image_only=True),\n",
    "                    SqueezeDimd(keys=[\"label\"], dim=2),\n",
    "                    Cropd(keys=[\"input\", \"label\"], pixel_width=1, pixel_height=1),\n",
    "                    AddChanneld(keys=[\"input\", \"label\"]), \n",
    "                    #Resized(keys=[\"input\", \"label\"], spatial_size=(512, 512))\n",
    "                    ])\n",
    "\n",
    "dataset=monai.data.Dataset(data=example_dict, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_transforms=SqueezeDimd(keys=['input', 'label'], dim=0), Invertd(\n",
    "                    keys=[\"input\"],\n",
    "                    transform=transforms,\n",
    "                    orig_keys=[\"input\"],\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_inverse=monai.data.Dataset(data=dataset, transform=post_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "applying transform (<monai.transforms.utility.dictionary.SqueezeDimd object at 0x7f9f18d083a0>, <monai.transforms.post.dictionary.Invertd object at 0x7f9f18a57070>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py:89\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=87'>88</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[0;32m---> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=88'>89</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items)\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=89'>90</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py:53\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mparameters)\n\u001b[0;32m---> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m transform(parameters)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000018vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mlist\u001b[39;49m(testing_inverse)[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py:97\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=94'>95</a>\u001b[0m     \u001b[39m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Subset(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39mindex)\n\u001b[0;32m---> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=96'>97</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(index)\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py:83\u001b[0m, in \u001b[0;36mDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=78'>79</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=79'>80</a>\u001b[0m \u001b[39mFetch single data item from `self.data`.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=80'>81</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=81'>82</a>\u001b[0m data_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index]\n\u001b[0;32m---> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/data/dataset.py?line=82'>83</a>\u001b[0m \u001b[39mreturn\u001b[39;00m apply_transform(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform, data_i) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m data_i\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py:113\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=110'>111</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=111'>112</a>\u001b[0m         _log_stats(data\u001b[39m=\u001b[39mdata)\n\u001b[0;32m--> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/monai/transforms/transform.py?line=112'>113</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapplying transform \u001b[39m\u001b[39m{\u001b[39;00mtransform\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform (<monai.transforms.utility.dictionary.SqueezeDimd object at 0x7f9f18d083a0>, <monai.transforms.post.dictionary.Invertd object at 0x7f9f18a57070>)"
     ]
    }
   ],
   "source": [
    "list(testing_inverse)[0][\"input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to nii \n",
    "\n",
    "\n",
    "\n",
    "d2n.convert_directory('/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved/0bzRUjziqI.dcm', '/home/ghita/milvuetap/ghita_exploration/data/example.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or no access: '/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved/0bzRUjziqI.dcm.nii'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py:42\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=40'>41</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=41'>42</a>\u001b[0m     stat_result \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(filename)\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=42'>43</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved/0bzRUjziqI.dcm.nii'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrossac/home/ghita/milvuetap/ghita_exploration/training_with_pl.ipynb#ch0000026vscode-remote?line=0'>1</a>\u001b[0m nib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved/0bzRUjziqI.dcm.nii\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py:44\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=41'>42</a>\u001b[0m     stat_result \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstat(filename)\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=42'>43</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=43'>44</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo such file or no access: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m stat_result\u001b[39m.\u001b[39mst_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='file:///home/ghita/libs/miniconda3/envs/trains/lib/python3.10/site-packages/nibabel/loadsave.py?line=45'>46</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ImageFileError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmpty file: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: '/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved/0bzRUjziqI.dcm.nii'"
     ]
    }
   ],
   "source": [
    "nib.load('/home/ghita/milvuetap/ghita_exploration/data/sarco dataset 1 & 2 improved/0bzRUjziqI.dcm.nii')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05f4eab8a48e0fb699a9c4d2ff5136ad6fe08181e3a510e3ac6da38970e2134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
